{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Denoising Diffusion Probabilistic Model\n",
    "\n",
    "## Overview\n",
    "The denoising diffusion probabilistic model (DDPM) can be viewed as a Markovian hierarchical\n",
    "variational autoencoder, where the forward encoder is a fixed linear Gaussian model, and\n",
    "we are interested in learning the reverse decoder.\n",
    "The idea is to progressively add Gaussian noise to the input data in the forward process\n",
    "until it is indistinguishable from the standard Gaussian noise and learn a denoising model\n",
    "in the backward process to reconstruct the input from the noise.\n",
    "\n",
    "Specifically, given the Markov property, the forward process can be factorized as\n",
    "$q(x_{1:T}|x_0)= \\prod_{t=1}^T q(x_t|x_{t-1})$, where $x_0$ is the input and $x_{1:T}$ are\n",
    "the latent variables given the input. We have\n",
    "$q(x_t|x_{t-1}) = \\mathcal{N}(x_t|\\sqrt{\\alpha_t} x_{t-1}, (1-\\alpha_t)I)$, and\n",
    "$\\alpha_t = 1-\\beta_t$ where $\\beta_t$ is the noise level added at each layer of the forward\n",
    "encoder.\n",
    "\n",
    "The join distribution of the reverse decoder is\n",
    "$p(x_{0:T})=p(x_T)\\prod_{t=1}^T p_\\theta(x_{t-1}|x_t)$ where $p(x_T)=\\mathcal{N}(x_T|0, I)$.\n",
    "Our goal is to learn the parameters $\\theta$ by maximizing the variational lower bound which\n",
    "is derived from the forward and reverse processes.\n",
    "\n",
    "## Method\n",
    "\n",
    "### Training\n",
    "Here are the training steps of the model:\n",
    "1. Given an input data batch $x_0 \\sim q(x_0)$, we uniformly sample\n",
    " the timestep index for each data in the batch that corresponds to the associated\n",
    "noisy sample, $t \\sim \\text{Uniform}(\\{1, \\dots, T\\})$.\n",
    "We also sample standard Gaussian noise for each data for the noise process,\n",
    "$\\epsilon_0 \\sim \\mathcal{N}(\\epsilon_0|0, I)$.\n",
    "2. In the forward encoder, we run the noise process $q(x_t|x_{t-1})$ for each sample in\n",
    "the input batch up to timestep $t$.\n",
    "Since the noise process is a linear Gaussian model, we can derive the\n",
    "closed-form formula of the noisy sample based on the input,\n",
    "initial noise $\\epsilon_0$ and timestep $t$:\n",
    "\n",
    "$$\n",
    "q(x_t|x_0) \\sim \\mathcal{N}(x_t|\\sqrt{\\bar{\\alpha}_t} x_0, (1 -\\bar{\\alpha}_t) I)\n",
    "$$\n",
    "\n",
    "where $\\bar α_t = \\prod_{i=1}^t α_i$. By using the reparameterization trick,\n",
    "we have:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 -\\bar{\\alpha}_t} \\epsilon_0\n",
    "$$\n",
    "\n",
    "3. In the reverse decoder, our goal is to learn the true conditional\n",
    "denoising distribution $q(x_{t-1}|x_t, x_0)$, which can be shown to be\n",
    "a Gaussian distribution\n",
    "$q(x_{t-1}|x_t, x_0) \\sim \\mathcal{N}(x_{t-1}|\\mu_q(x_t, x_0), \\Sigma_q(t))$, where:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_q(x_t, x_0) &= \\frac{\\sqrt{\\alpha_t} (1 - \\bar{\\alpha}_{t-1}) x_t + \\sqrt{\\bar{\\alpha}_{t-1}} (1 - \\alpha_t) x_0}{1 - \\bar{\\alpha}_t}\\\\\n",
    "\\Sigma_q(t) &= \\sigma_q^2(t)I = \\frac{(1-\\alpha_t)(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}I\n",
    "\\end{aligned}\n",
    "$$\n",
    "We can also derive the mean of the conditional distribution as below by utilizing the reparameterization trick:\n",
    "\n",
    "$$\n",
    "\\mu_q(x_t, x_0) = \\frac{1}{\\sqrt{\\alpha_t}}x_t - \\frac{1 - \\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}\\sqrt{\\alpha_t}}\\epsilon_0\n",
    "$$\n",
    "\n",
    "Since we don't have access to the ground-truth input $x_0$ and the\n",
    "initial noise $\\epsilon_0$ during the reverse process, we define\n",
    "the decoder's denoising model $p_\\theta(x_{t-1}|x_t, t)$ as Gaussian\n",
    "distribution with the following mean and covariance:\n",
    "\n",
    "$$\n",
    "\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}}x_t - \\frac{1 - \\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}\\sqrt{\\alpha_t}} \\hat{\\epsilon}_\\theta (x_t, t)\n",
    "$$\n",
    "\n",
    "where the noise is parameterized by a neural network with parameters $\\theta$.\n",
    "\n",
    "4. It can be shown that maximizing the variational lower bound boils down\n",
    "to minimizing the following loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{simple}}(θ) =\n",
    "\\mathbb{E}_{t \\sim \\mathcal{U}[1,T],\\, ε_0 \\sim \\mathcal{N}(0,I),\\, x_0 \\sim \\text{data}} \\frac{1}{2\\sigma_q^2(t)} \\frac{(1-\\alpha_t)^2}{(1 - \\bar{\\alpha}_t)\\alpha_t} \\left[\n",
    "\\left\\| ε_0 - \\hat ε_θ(x_t, t) \\right\\|^2_2\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "### Sampling\n",
    "After the training, to generate new samples, execute the reverse process:\n",
    "1. Sample a random sample $x_T \\sim \\mathcal{N}(x_T|0, I)$\n",
    "2. For each step of the reverse process $t=T, \\dots, 1$:\n",
    "    - If $t > 1$ sample a noise $\\epsilon \\sim \\mathcal{N}(\\epsilon|0, I)$, otherwise set $\\epsilon=0$\n",
    "    - Compute the denoised sample $x_{t-1}=\\mu_\\theta(x_t, t) + \\sigma_q(t)\\epsilon$\n"
   ],
   "id": "ef73c31280cd1e9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8e95ec41d1bbc35e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
